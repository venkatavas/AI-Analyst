# Usage Guide

## Quick Start

### ğŸš€ One-Command Complete Pipeline
```bash
# Windows
.\run_demo.bat

# Cross-platform Python
python run_demo.py
```
**Executes all 6 steps automatically:**
1. Data ingestion (687 records)
2. Data cleaning and quality assessment  
3. Statistical transformation and feature engineering
4. Dual-API AI insights (Groq + HuggingFace)
5. ML clustering analysis (5 clusters)
6. Anomaly detection (IQR-based outliers)

**Expected Complete Output:**
```
========================================
ğŸš€ RTGS CLI - Complete Pipeline Demo
========================================
Production-Ready Governance Analytics
Dual-API AI Integration + ML Analytics
========================================

[â†’] Step 1: Ingest raw dataset (687 records)
[âœ“] Step 1: Ingest raw dataset (687 records) completed successfully

[â†’] Step 2: Clean standardized dataset
[âœ“] Step 2: Clean standardized dataset completed successfully

[â†’] Step 3: Transform cleaned dataset
[âœ“] Step 3: Transform cleaned dataset completed successfully

[â†’] Step 4: Generate dual-API insights (Groq + HuggingFace)
[âœ“] Step 4: Generate dual-API insights (Groq + HuggingFace) completed successfully

[â†’] Step 5: ML clustering analysis (SimpleMlAgent)
[âœ“] Step 5: ML clustering analysis (SimpleMlAgent) completed successfully

[â†’] Step 6: Anomaly detection (IQR-based outliers)
[âœ“] Step 6: Anomaly detection (IQR-based outliers) completed successfully

========================================
[âœ“] Complete Pipeline Demo Finished!
========================================
ğŸ“Š Analytics Results Generated:
   ğŸ“„ Schema Analysis: outputs/schema_summary.json
   ğŸ§¹ Data Quality: outputs/cleaning_report.json
   ğŸ”„ Transformations: outputs/transformation_report.json
   ğŸ¤– Dual-API Insights: outputs/insights_report.md
   ğŸ¯ ML Clusters: outputs/clusters.json
   ğŸš¨ Anomalies: outputs/anomalies.json
   ğŸ“ˆ Visualizations: outputs/trend.png

ğŸ‰ Production-ready governance analytics complete!
ğŸ“Š 687 records processed across 630+ wards
ğŸ¤– Dual-API AI analysis (Groq + HuggingFace)
ğŸ¯ 5 clusters identified, 3 outliers detected
```

## Step-by-Step Usage

### 1. Ingest Command
Process raw CSV data and generate schema information.

```bash
python cli.py ingest data/raw/Illiterate_Khammam_Rural.csv
```

**Expected Output:**
```
[â†’] INGEST: Starting data ingestion process
[â†’] LOAD: Reading CSV file: Illiterate_Khammam_Rural.csv
[âœ“] Loaded 687 rows and 7 columns
[â†’] STANDARDIZE: Converting column names to lowercase with underscores
[âœ“] Column names standardized
[â†’] ANALYZE: Generating schema summary

ğŸ“Š Dataset Overview:
   â€¢ Total Rows: 687
   â€¢ Total Columns: 7

ğŸ“‹ Column Details:
   â€¢ district (object): 687 non-null (0.0% missing), 1 unique
   â€¢ mandal (object): 687 non-null (0.0% missing), 46 unique
   â€¢ muncipality (object): 687 non-null (0.0% missing), 46 unique
   â€¢ wardname (object): 687 non-null (0.0% missing), 630 unique
   â€¢ female (int64): 687 non-null (0.0% missing), 312 unique
   â€¢ male (int64): 687 non-null (0.0% missing), 298 unique
   â€¢ transgender (int64): 687 non-null (0.0% missing), 4 unique

[â†’] SAVE: Saving processed files
[âœ“] Successfully processed Illiterate_Khammam_Rural.csv
   ğŸ“ Standardized data: data\raw\Illiterate_Khammam_Rural_standardized.csv
   ğŸ“„ Schema JSON: outputs\schema_summary.json
   ğŸ“Š Schema CSV: outputs\schema_summary.csv
```

### 2. Clean Command
Remove duplicates, handle missing values, and detect outliers.

```bash
python cli.py clean data/raw/Illiterate_Khammam_Rural_standardized.csv
```

**Expected Output:**
```
[â†’] CLEAN: Starting data cleaning process
[â†’] LOAD: Reading standardized file: Illiterate_Rangareddy_Urban_Area_standardized.csv
[âœ“] Loaded 126 rows for cleaning
[â†’] PROCESS: Applying data cleaning operations
[âœ“] Cleaning completed - 126 rows remaining

ğŸ§¹ Cleaning Summary:
   â€¢ Duplicates removed: 0
   â€¢ Missing values filled: 0
   â€¢ Outliers replaced: 0

[â†’] SAVE: Saving cleaned data and report
[âœ“] Successfully cleaned Illiterate_Khammam_Rural_standardized.csv
   ğŸ“ Cleaned data: data\cleaned\Illiterate_Khammam_Rural_standardized_cleaned.csv
   ğŸ“„ Cleaning report: outputs\cleaning_report.json
```

### 3. Transform Command
Aggregate data by ward and calculate derived metrics.

```bash
python cli.py transform data/cleaned/Illiterate_Khammam_Rural_standardized_cleaned.csv
```

**Expected Output:**
```
[â†’] TRANSFORM: Starting data transformation process
[â†’] LOAD: Reading cleaned file: Illiterate_Khammam_Rural_standardized_cleaned.csv
[âœ“] Loaded 687 rows for transformation
[â†’] PROCESS: Applying transformations and aggregations
[âœ“] Transformations completed

ğŸ”„ Transformation Summary:
   â€¢ Total illiterates: 6,789
   â€¢ Male illiterates: 2,567
   â€¢ Female illiterates: 4,222
   â€¢ Top ward: Nadergul (456 illiterates)

[â†’] SAVE: Saving transformed data and generating visualization
[âœ“] Successfully transformed Illiterate_Khammam_Rural_standardized_cleaned.csv
   ğŸ“ Transformed data: data\cleaned\Illiterate_Khammam_Rural_cleaned_transformed.csv
   ğŸ“„ Transformation report: outputs\transformation_report.json
   ğŸ“Š Visualization: outputs\trend.png
```

### 4. Insights Command
Generate dual-API AI-powered policy recommendations and visualizations.

```bash
python cli.py insights data/cleaned/Illiterate_Khammam_Rural_standardized_cleaned_transformed.csv
```

**Expected Output:**
```
[â†’] INSIGHTS: Starting insights generation process
[â†’] LOAD: Reading transformed file: Illiterate_Khammam_Rural_standardized_cleaned_transformed.csv
[âœ“] Loaded 687 rows for analysis
[â†’] ANALYZE: Generating insights and policy recommendations
[âœ“] Insights report generated
[â†’] AI_ENHANCE: Generating dual-API AI summary (Groq + HuggingFace)
[âœ“] AI summary added with narrative and mathematical analysis
[â†’] SAVE: Saving insights report
[âœ“] Successfully generated insights report
   ğŸ“„ Insights report saved: outputs\insights_report.md

Quick Insights Summary:
   â€¢ Total illiterates analyzed: 1,049
   â€¢ High-risk wards identified: 15
   â€¢ Gender disparity detected: 62.3% female
   â€¢ Policy recommendations: 8 actionable items
```

### 5. ML Clustering Command
Analyze ward clustering patterns using SimpleMlAgent.

```bash
python cli.py cluster data/cleaned/Illiterate_Khammam_Rural_standardized_cleaned_transformed.csv
```

**Expected Output:**
```
[â†’] CLUSTER: Starting ML clustering analysis
[â†’] LOAD: Reading transformed data for clustering
[âœ“] Loaded 630 wards for analysis
[â†’] PROCESS: Applying k-means clustering (k=5)
[âœ“] Clustering completed - 5 clusters identified

ğŸ“Š Clustering Results:
   â€¢ Cluster 1: 198 wards (avg: 869 illiterates)
   â€¢ Cluster 2: 24 wards (avg: 3,001 illiterates) - HIGH RISK
   â€¢ Cluster 3: 198 wards (avg: 465 illiterates)
   â€¢ Cluster 4: 122 wards (avg: 1,286 illiterates)
   â€¢ Cluster 5: 88 wards (avg: 1,903 illiterates)

[âœ“] Clustering analysis saved: outputs\clusters.json
```

### 6. Anomaly Detection Command
Detect statistical outliers in governance data.

```bash
python cli.py anomalies data/cleaned/Illiterate_Khammam_Rural_standardized_cleaned_transformed.csv
```

**Expected Output:**
```
[â†’] ANOMALIES: Starting anomaly detection analysis
[â†’] LOAD: Reading data for outlier detection
[âœ“] Loaded 630 wards for analysis
[â†’] PROCESS: Applying IQR-based anomaly detection
[âœ“] Anomaly detection completed

ğŸš¨ Anomalies Detected:
   â€¢ ASWAPURAM: 2,617 illiterates (+150% deviation)
   â€¢ BAYYARAM: 3,053 illiterates (+191% deviation)
   â€¢ CHALLA SAMUDRAM: 3,323 illiterates (+217% deviation)

ğŸ“Š Analysis Summary:
   â€¢ Total outliers: 3 wards
   â€¢ Threshold (Q3 + 1.5*IQR): 1,750 illiterates
   â€¢ Mean deviation: +186%

[âœ“] Anomaly analysis saved: outputs\anomalies.json
```

Quick Insights Summary:
   [â†’] Key finding: Female illiteracy is 75.7% higher than male overall.
   [â†’] Recommendation: Target literacy programs in wards: Nadergul, Revenue Ward No 2, Revenue Ward No 8.
```

## Command Options

### Ingest Command
```bash
python cli.py ingest <input_file> [options]
```

**Arguments:**
- `input_file`: Path to the raw CSV file

**Options:**
- `-o, --output-dir`: Directory to save standardized files (default: `data/cleaned`)

### Clean Command
```bash
python cli.py clean <input_file> [options]
```

**Arguments:**
- `input_file`: Path to the standardized CSV file

**Options:**
- `-o, --output-dir`: Directory to save cleaned files (default: `data/cleaned`)

### Transform Command
```bash
python cli.py transform <input_file> [options]
```

**Arguments:**
- `input_file`: Path to the cleaned CSV file

**Options:**
- `-o, --output-dir`: Directory to save transformed files (default: `data/cleaned`)

### Insights Command
```bash
python cli.py insights <input_file>
```

**Arguments:**
- `input_file`: Path to the transformed CSV file

## File Naming Conventions

The pipeline follows consistent naming patterns:

1. **Raw data**: `original_filename.csv`
2. **Standardized**: `original_filename_standardized.csv`
3. **Cleaned**: `original_filename_standardized_cleaned.csv`
4. **Transformed**: `original_filename_cleaned_transformed.csv`

## Troubleshooting

### Common Issues

#### 1. Virtual Environment Not Activated
**Error:** `'python' is not recognized as an internal or external command`

**Solution:**
```bash
.\venv\Scripts\activate
```

#### 2. Missing Dependencies
**Error:** `ModuleNotFoundError: No module named 'pandas'`

**Solution:**
```bash
pip install -r requirements.txt
```

#### 3. File Not Found
**Error:** `Input file 'filename.csv' not found`

**Solution:**
- Check file path is correct
- Ensure file exists in the specified directory
- Use absolute paths if relative paths fail

#### 4. Permission Errors
**Error:** `PermissionError: [Errno 13] Permission denied`

**Solution:**
- Close any Excel files that might have the CSV open
- Run command prompt as administrator
- Check file permissions

#### 5. Encoding Issues
**Error:** `UnicodeDecodeError: 'charmap' codec can't decode`

**Solution:**
- Ensure CSV files are saved in UTF-8 encoding
- Check for special characters in data

### Performance Tips

#### For Large Datasets (>100K rows)
- Increase available RAM
- Process in chunks using custom batch sizes
- Monitor disk space in outputs directory

#### For Slow Processing
- Close unnecessary applications
- Use SSD storage for faster I/O
- Check antivirus software isn't scanning files

### Getting Help

#### CLI Help
```bash
python cli.py --help
python cli.py ingest --help
python cli.py clean --help
python cli.py transform --help
python cli.py insights --help
```

#### Debug Mode
Add verbose logging by modifying the CLI commands to include debug prints.

#### Log Files
Check the console output for detailed error messages and progress indicators.
